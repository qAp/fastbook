{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpass and Pytorch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch's Autograd allows the automatic differention of complex computations and is central to the backpropagation of neural networks.  This post focuses on the foundations of computing gradients using this functionality, points out the things to watch out for, and provides an example walk-thru to consolidate the points  being made. No reference is made to an actual neural network model, but the ideas should be applicable to them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Nomenclature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial{f}}{\\partial{x}}\n",
    "$$\n",
    "is called:\n",
    "* The (first) partial derivative of $f$ with respect to $x$.\n",
    "* The (first) partial derivative of $f$ w.r.t $x$.\n",
    "* The gradient of $f$.\n",
    "* The $x$-component of the gradient of $f$.\n",
    "* The gradient of $f$ on $x$.\n",
    "* The gradient on $x$.\n",
    "* The gradient on $x$ w.r.t $f$.\n",
    "\n",
    "In deep learning, *gradient* is more commonly used than *derivative*, so gradient will be used in general in this post, but hopefully the explicit mathematical symbols shown alongside will help avoid confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to compute gradient: basic howto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To compute the gradient in Pytorch, it's generally sufficient to simply:\n",
    "1. Set `requires_grad` to `True`.\n",
    "2. Call the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3., requires_grad=True), tensor(1.))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The gradient that is computed by `x.backward()` here is:\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dx}\\bigr\\rvert_{x=3} = 1\n",
    "$$\n",
    "\n",
    "and it is stored in the `grad` attribute of `x`, `x.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a piece of code, statements which involve, either directly or indirectly, one or more tensors with `requires_grad=True`, form a computation graph [1][karpathy].  The computation graph keeps track of the computation described by the statements and provides the information needed for Autograd to compute the gradients when `backward` is called.\n",
    "\n",
    "[karpathy]: https://cs231n.github.io/optimization-2/#backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the following statements, because none of the tensors have `requires_grad=True`, there isn't information to allow the computation of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.), tensor(8193.0840), tensor(8195.0840))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(9.)\n",
    "y = x**2 + x + torch.exp(x)\n",
    "z = y + 2\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Whereas in the following statements, there is a computation graph, which you can tell because when printed, `requires_grad=True` or a `grad_fn` attribute is displayed alongside their values.  Notice here that `x` cannot have gradients on it computed, nor its `backward` called, because it has neither a `requires_grad=True` nor a `grad_fn`; it's not part of a computation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.),\n",
       " tensor(8193.0840, requires_grad=True),\n",
       " tensor(8195.0840, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(9.)\n",
    "y = x**2 + x + torch.exp(x)\n",
    "y.requires_grad_()\n",
    "z = y + 2\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In general, statements leading up to a `backward` call constitute what's called a *forward propagation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Can only get gradients on leaf variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's examine the effect of `z.backward()` on the `grad` attributes of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None None\n",
      "None tensor(1.) None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad, y.grad, z.grad)\n",
    "z.backward()\n",
    "print(x.grad, y.grad, z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It can be seen that `z` has no `grad`, even though it is expected that the gradient of `z` on `z`, $\\frac{dz}{dz}$, should be $1$.  This is because `z` is not what is considered a leaf variable by Pytorch.  Leaf variables are the earliest variables in a computation graph; often they are the inputs of a computation.  Pytorch only makes the gradients on leaf variables available after a `backward` call, in order to save memory.  It appears that one way to distinguish a leaf variable from one that isn't, an intermediary variable, is to print them.  A leaf variable has `requires_grad=True` printed after its value, while an intermediary variable has a `grad_fn` of some sort printed after its value.\n",
    "\n",
    "If you do want to get the gradients on intermediary variables, you need to use hooks, but this is outside the scope of this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Resetting the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's important to note that `x.grad` accumulates additively with each successful call of the `backward` method, so care needs to be taken to reset the gradient whenever appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For example, it is expected that $\\frac{dx}{dx}$ should always be $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "x.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are various ways to reset the gradient: `grad.zeros_()`, `grad.data.zero_`, and `grad = None`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "x.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "x.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "x.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Computation graph is flushed after `backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the previous section, `x.backward()` was called multiple times without any problems (except for the need to reset `grad` in order for it to make sense).  \n",
    "\n",
    "However, as soon as additional computations are done on `x`, the `backward()` method of the result of these computations can no longer be called consecutively.  For example, take the following computation that takes the square of $x$:\n",
    "$$\n",
    "m = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "m = x**2\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Differentiating with respect to $x$:\n",
    "$$\n",
    "\\frac{dm}{dx} = \\frac{d}{dx} x^2 = 2x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, repeating the above cell again raises a `RuntimeError`.  According to clarification in this [thread](https://discuss.pytorch.org/t/runtimeerror-trying-to-backward-through-the-graph-a-second-time-but-the-buffers-have-already-been-freed-specify-retain-graph-true-when-calling-backward-the-first-time/6795/2), this is because all the 'intermediary results' of the computations are deleted after the first `backward` call, so another backward call cannot be done, as it needs these intermediary results.  The reason for the deletion of these intermediary results by default is to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d98783714e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "m.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a computation that is very small, like the current one, this can be resolved by simply repeating the computation, hence making the intermediary results available again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = x**2\n",
    "x.grad.zero_()\n",
    "m.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, for large computations which take a long time, it's better to use `backward(retain_graph=True)` to tell Pytorch not to delete those intermediary results, so that another `backward` call can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = x**2\n",
    "x.grad.zero_()\n",
    "m.backward(retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "m.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Practice: staged computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's now consider a larger computation and do the backpass on it in stages , to practice the ideas described so far.  This is in the same vein as in [1][karpathy], except Pytorch is used, so there is no need to first find the analytical expressions of the gradients.\n",
    "\n",
    "Consider the following computation:\n",
    "\n",
    "$$\n",
    "x = 3\\\\\n",
    "m = x^2\\\\\n",
    "y = m + x\\\\\n",
    "p = 4y\\\\\n",
    "q = -2y\n",
    "$$\n",
    "\n",
    "Note that each equation is represented as a gate in a computation graph, with the variable on the LHS being the gate output, and the variable(s) on the RHS being the gate input(s).\n",
    "\n",
    "[karpathy]: https://cs231n.github.io/optimization-2/#backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img alt=\"Simple computation graph to illustrate fundamentals of autograd in Pytorch\" width=\"400\" src=\"jacks_images/example_computation_graph.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Forward pass through the computation graph.  The values of all variables in the computation are printed out, as these might be needed during the backward pass.  The values are:\n",
    "$$\n",
    "x = 3 \\\\\n",
    "m = 9 \\\\\n",
    "y = 12 \\\\\n",
    "p = 48 \\\\\n",
    "q = -24\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=3.0 m=9.0 y=12.0 p=48.0 q=-24.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "m = x**2\n",
    "y = m + x\n",
    "p = 4*y\n",
    "q = -2*y\n",
    "print(f'x={x} m={m} y={y} p={p} q={q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here, the computation graph has two final branches, one that ends with $p$, and one that ends with $q$.  Let's first backpass through the gate along the former:\n",
    "\n",
    "$$\n",
    "y=12\\\\\n",
    "p=4y\n",
    "$$\n",
    "\n",
    "Notice that, because Pytorch only makes the leaf variables' gradients available, it's necessary to use a computation graph that starts with $y$, using its value obtained during the forward pass (even though here its value doesn't matter in the backpass):\n",
    "\n",
    "$$\n",
    "\\frac{dp}{dy}\\bigr\\rvert_{y=12} = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(12., requires_grad=True)\n",
    "p = 4*y\n",
    "p.backward()\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Similarly for $q = -2y$:\n",
    "$$\n",
    "\\frac{dq}{dy}\\bigr\\rvert_{y=12} = -2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(12., requires_grad=True)\n",
    "q = -2*y\n",
    "q.backward()\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The remaining part of the computation graph to backpass through can be expressed as:\n",
    "$$\n",
    "x = 3\\\\\n",
    "m = x^2\\\\\n",
    "y = m + x\n",
    "$$\n",
    "Instead of back passing first through the y gate and then the m gate, $\\frac{dy}{dx}$ can be more directly obtained by using the expression\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dm}{dx} + \\frac{dx}{dx}\n",
    "$$\n",
    "and the facts that: \n",
    "* `x.grad` accumulates additively, so calculating the RHS of the above equation amounts to calling `m.backward` followed by `x.backward()`.\n",
    "* `x.backward()` doesn't require any intermediary results, since `x` is already a leaf variable.\n",
    "\n",
    "The gradient should be:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx}\\bigr\\rvert_{x=3} = \\frac{dm}{dx}\\bigr\\rvert_{x=3} + \\frac{dx}{dx}\\bigr\\rvert_{x=3} = 2\\cdot 3 + 1 = 7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm/dx = 6.0\n",
      "dy/dx = dm/dx + dx/dx = 7.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "m = x**2\n",
    "m.backward()\n",
    "print(f'dm/dx = {x.grad}')\n",
    "x.backward()\n",
    "print(f'dy/dx = dm/dx + dx/dx = {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Having backpropagated through all the gates and noted the gradients at each, the chain rule for derivatives can be used to construct the gradient for the whole computation graph.  In this case, there will be two overall gradients, $\\frac{dp}{dx}$ and $\\frac{dq}{dx}$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{dp}{dx} = \\frac{dp}{dy}\\frac{dy}{dx} = 4 \\cdot 7 = 28\\\\\n",
    "\\frac{dq}{dx} = \\frac{dq}{dy}\\frac{dy}{dx} = -2 \\cdot 7 = -14\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the result of the staged computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section above, the computation of the gradients $\\frac{dp}{dx}$ and $\\frac{dq}{dx}$ is broken up into small steps.  In this section, `backward` is called for the whole graph once, so that the backpass is done in one big step instead.  This is the normal way to use `backward`, which is to describe whatever computation is desired through normal statements, and at the end of the statements, call the `backward` method of the computation's output to back propagate all the way to get the gradients on the leaf variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=3.0 m=9.0 y=12.0 p=48.0 q=-24.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "m = x**2\n",
    "y = m + x\n",
    "p = 4*y\n",
    "q = -2*y\n",
    "print(f'x={x} m={m} y={y} p={p} q={q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen, either from the chain rule or the computation graph, that $\\frac{dp}{dx}$ and $\\frac{dq}{dx}$ have $\\frac{dy}{dx}$ in common.  Therefore, it's necessary to have `retain_graph=True` when calling `backward` to obtain one before calling `backward` again to obtain the other, otherwise an error will result, because $\\frac{dy}{dx}$, being a intermediary result here, will have been cleared from the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "p.backward(retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-14.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "q.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting $\\frac{dp}{dx}$ and $\\frac{dq}{dx}$ are consistent with those obtained by staged computation in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Intuitive Understanding of Backpropagation by Andrej Karpthay]( https://cs231n.github.io/optimization-2/) \n",
    " \n",
    "* [Pytorch Autograd Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "\n",
    "* [*The Gradient*, Chapter4, Fastbook](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
